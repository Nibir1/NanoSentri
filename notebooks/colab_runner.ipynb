{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sadYD3P9jy7T"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements_colab.txt\n",
        "torch>=2.2.0\n",
        "transformers>=4.38.0\n",
        "datasets>=2.17.0\n",
        "peft>=0.18.1\n",
        "bitsandbytes>=0.43.0\n",
        "accelerate>=0.30.0\n",
        "scipy>=1.11.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/trl"
      ],
      "metadata": {
        "id": "4KsnbHed6tq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Create the directory for the script\n",
        "os.makedirs('src', exist_ok=True)"
      ],
      "metadata": {
        "id": "yKEAi-3qppcG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/train_colab.py\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[NanoSentri-Train]: {msg}\")\n",
        "\n",
        "def format_phi3_prompt(sample):\n",
        "    instruction = sample['instruction']\n",
        "    context = sample.get('input', '')\n",
        "    response = sample['output']\n",
        "\n",
        "    if context:\n",
        "        user_content = f\"{instruction}\\n\\nTechnical Context:\\n{context}\"\n",
        "    else:\n",
        "        user_content = instruction\n",
        "\n",
        "    text = f\"<|user|>\\n{user_content} <|end|>\\n<|assistant|>\\n{response} <|end|>\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Phi-3 QLoRA Trainer\")\n",
        "    parser.add_argument(\"--data_path\", type=str, required=True, help=\"Path to JSONL dataset\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./phi3-vaisala-adapter\", help=\"Where to save adapters\")\n",
        "    parser.add_argument(\"--base_model\", type=str, default=\"microsoft/Phi-3-mini-4k-instruct\", help=\"HF Model ID\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    log(f\"Initializing Training Pipeline on {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "    log(f\"Loading base model: {args.base_model}...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.base_model,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\"\n",
        "    )\n",
        "\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.base_model, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.unk_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\"]\n",
        "    )\n",
        "\n",
        "    log(\"Loading and formatting dataset...\")\n",
        "    dataset = load_dataset(\"json\", data_files=args.data_path, split=\"train\")\n",
        "    dataset = dataset.map(format_phi3_prompt)\n",
        "\n",
        "    log(f\"Sample formatted entry:\\n{dataset[0]['text']}\")\n",
        "\n",
        "    # Tokenize the dataset with truncation\n",
        "    log(\"Tokenizing dataset...\")\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=512,\n",
        "            return_tensors=None\n",
        "        )\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=10,\n",
        "        max_steps=100,\n",
        "        save_steps=50,\n",
        "        fp16=False,\n",
        "        bf16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        report_to=\"none\",\n",
        "        gradient_checkpointing=True,\n",
        "    )\n",
        "\n",
        "    # Data collator for language modeling\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,  # Not masked language modeling\n",
        "    )\n",
        "\n",
        "    # SFTTrainer for TRL 0.7.10 - minimal parameters\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        peft_config=peft_config,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    log(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    log(f\"Training complete. Saving adapters to {args.output_dir}\")\n",
        "    trainer.model.save_pretrained(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Q-OPB-oxkVZg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements_colab.txt\n",
        "!pip install flash_attn --no-build-isolation # Optional speedup"
      ],
      "metadata": {
        "id": "JrHyIqAmkrZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/train_colab.py --data_path \"vaisala_synthetic_train.jsonl\" --output_dir \"phi3-vaisala-adapter\""
      ],
      "metadata": {
        "id": "7Wgl1peyl91i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create a zip file of the adapter folder\n",
        "shutil.make_archive('phi3-vaisala-adapter', 'zip', 'phi3-vaisala-adapter')\n",
        "\n",
        "# Trigger the browser download\n",
        "files.download('phi3-vaisala-adapter.zip')"
      ],
      "metadata": {
        "id": "eg5_QoTamGW3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}