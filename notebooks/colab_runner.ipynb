{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sadYD3P9jy7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f9077d9-e404-4fd3-e877-1c9d346238ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing requirements_colab.txt\n"
          ]
        }
      ],
      "source": [
        "%%writefile requirements_colab.txt\n",
        "torch>=2.2.0\n",
        "transformers>=4.38.0\n",
        "datasets>=2.17.0\n",
        "peft>=0.18.1\n",
        "bitsandbytes>=0.43.0\n",
        "accelerate>=0.30.0\n",
        "scipy>=1.11.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/trl"
      ],
      "metadata": {
        "id": "4KsnbHed6tq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Create the directory for the script\n",
        "os.makedirs('src', exist_ok=True)"
      ],
      "metadata": {
        "id": "yKEAi-3qppcG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/train_colab.py\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[NanoSentri-Train]: {msg}\")\n",
        "\n",
        "def format_phi3_prompt(sample):\n",
        "    instruction = sample['instruction']\n",
        "    context = sample.get('input', '')\n",
        "    response = sample['output']\n",
        "\n",
        "    if context:\n",
        "        user_content = f\"{instruction}\\n\\nTechnical Context:\\n{context}\"\n",
        "    else:\n",
        "        user_content = instruction\n",
        "\n",
        "    text = f\"<|user|>\\n{user_content} <|end|>\\n<|assistant|>\\n{response} <|end|>\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Phi-3 QLoRA Trainer\")\n",
        "    parser.add_argument(\"--data_path\", type=str, required=True, help=\"Path to JSONL dataset\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./phi3-vaisala-adapter\", help=\"Where to save adapters\")\n",
        "    parser.add_argument(\"--base_model\", type=str, default=\"microsoft/Phi-3-mini-4k-instruct\", help=\"HF Model ID\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    log(f\"Initializing Training Pipeline on {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "    log(f\"Loading base model: {args.base_model}...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.base_model,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\"\n",
        "    )\n",
        "\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.base_model, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.unk_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\"]\n",
        "    )\n",
        "\n",
        "    log(\"Loading and formatting dataset...\")\n",
        "    dataset = load_dataset(\"json\", data_files=args.data_path, split=\"train\")\n",
        "    dataset = dataset.map(format_phi3_prompt)\n",
        "\n",
        "    log(f\"Sample formatted entry:\\n{dataset[0]['text']}\")\n",
        "\n",
        "    # Tokenize the dataset with truncation\n",
        "    log(\"Tokenizing dataset...\")\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=512,\n",
        "            return_tensors=None\n",
        "        )\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=10,\n",
        "        max_steps=100,\n",
        "        save_steps=50,\n",
        "        fp16=False,\n",
        "        bf16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        report_to=\"none\",\n",
        "        gradient_checkpointing=True,\n",
        "    )\n",
        "\n",
        "    # Data collator for language modeling\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,  # Not masked language modeling\n",
        "    )\n",
        "\n",
        "    # SFTTrainer for TRL 0.7.10 - minimal parameters\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        peft_config=peft_config,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    log(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    log(f\"Training complete. Saving adapters to {args.output_dir}\")\n",
        "    trainer.model.save_pretrained(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Q-OPB-oxkVZg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "717e0e3d-fd12-433f-911f-20a1105aa48f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing src/train_colab.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements_colab.txt\n",
        "!pip install flash_attn --no-build-isolation # Optional speedup"
      ],
      "metadata": {
        "id": "JrHyIqAmkrZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/train_colab.py --data_path \"vaisala_synthetic_train.jsonl\" --output_dir \"phi3-vaisala-adapter\""
      ],
      "metadata": {
        "id": "7Wgl1peyl91i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdede054-1e2b-44c3-f945-1b8ad4b63718"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2026-01-17 16:17:50.920343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1768666670.940514    1466 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1768666670.946664    1466 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1768666670.961582    1466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768666670.961605    1466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768666670.961609    1466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1768666670.961614    1466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2026-01-17 16:17:50.966280: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "[NanoSentri-Train]: Initializing Training Pipeline on Tesla T4\n",
            "[NanoSentri-Train]: Loading base model: microsoft/Phi-3-mini-4k-instruct...\n",
            "config.json: 100% 967/967 [00:00<00:00, 8.02MB/s]\n",
            "configuration_phi3.py: 11.2kB [00:00, 6.94MB/s]\n",
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- configuration_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "modeling_phi3.py: 73.2kB [00:00, 103MB/s]\n",
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-4k-instruct:\n",
            "- modeling_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "model.safetensors.index.json: 16.5kB [00:00, 72.3MB/s]\n",
            "Fetching 2 files:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/2.67G [00:00<?, ?B/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 0.00/4.97G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 690k/4.97G [00:02<4:09:31, 332kB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 3.00M/2.67G [00:02<30:56, 1.44MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 4.03M/2.67G [00:03<36:28, 1.22MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 5.39M/4.97G [00:03<43:28, 1.90MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   0% 9.46M/2.67G [00:03<13:35, 3.26MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   0% 20.5M/4.97G [00:04<11:59, 6.88MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 28.8M/2.67G [00:06<07:04, 6.22MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 33.2M/4.97G [00:06<13:44, 5.99MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   1% 55.3M/4.97G [00:07<07:15, 11.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 50.9M/2.67G [00:08<05:55, 7.36MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   2% 75.8M/4.97G [00:09<07:52, 10.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   3% 143M/4.97G [00:14<06:53, 11.7MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 118M/2.67G [00:14<04:27, 9.53MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 148M/2.67G [00:15<03:08, 13.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   4% 194M/4.97G [00:15<04:35, 17.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 201M/2.67G [00:16<02:06, 19.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   5% 248M/4.97G [00:16<03:12, 24.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   6% 303M/4.97G [00:18<02:54, 26.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 249M/2.67G [00:18<01:58, 20.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 316M/2.67G [00:20<01:40, 23.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   8% 415M/4.97G [00:20<02:21, 32.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 362M/2.67G [00:21<01:17, 29.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:   9% 471M/4.97G [00:21<01:50, 40.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 376M/2.67G [00:21<01:17, 29.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  11% 541M/4.97G [00:24<02:28, 29.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 443M/2.67G [00:25<01:30, 24.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  12% 603M/4.97G [00:25<01:51, 39.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 464M/2.67G [00:25<01:28, 24.9MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 474M/2.67G [00:26<01:29, 24.6MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 565M/2.67G [00:26<00:43, 48.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  15% 733M/4.97G [00:26<01:21, 51.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 637M/2.67G [00:30<01:11, 28.4MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 730M/2.67G [00:31<00:43, 44.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  16% 802M/4.97G [00:31<02:11, 31.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 777M/2.67G [00:31<00:36, 52.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 791M/2.67G [00:32<00:36, 51.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  17% 869M/4.97G [00:32<01:47, 38.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 870M/2.67G [00:37<01:08, 26.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 882M/4.97G [00:37<03:28, 19.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  18% 894M/4.97G [00:37<03:20, 20.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 915M/2.67G [00:37<00:55, 31.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  19% 934M/4.97G [00:37<02:28, 27.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 982M/2.67G [00:43<01:23, 20.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  20% 998M/4.97G [00:43<03:42, 17.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  21% 1.06G/4.97G [00:43<02:37, 24.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.09G/2.67G [00:44<00:48, 32.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  22% 1.09G/4.97G [00:44<02:12, 29.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.13G/2.67G [00:44<00:42, 36.2MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.16G/2.67G [00:49<01:15, 20.0MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.27G/2.67G [00:50<00:41, 33.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  23% 1.16G/4.97G [00:50<03:32, 17.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 1.19G/4.97G [01:03<03:30, 17.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 1.27G/2.67G [01:03<00:41, 33.9MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  24% 1.22G/4.97G [01:05<07:29, 8.35MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 1.34G/2.67G [01:05<01:54, 11.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  25% 1.26G/4.97G [01:05<05:48, 10.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 1.40G/2.67G [01:05<01:20, 15.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  26% 1.31G/4.97G [01:09<05:13, 11.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 1.44G/2.67G [01:11<01:39, 12.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  29% 1.44G/4.97G [01:11<03:02, 19.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.48G/2.67G [01:12<01:17, 15.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  30% 1.50G/4.97G [01:12<02:15, 25.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 1.50G/2.67G [01:14<01:20, 14.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  31% 1.55G/4.97G [01:14<02:19, 24.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 1.64G/4.97G [01:14<01:26, 38.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  33% 1.66G/4.97G [01:14<01:23, 39.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 1.72G/4.97G [01:17<01:31, 35.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  35% 1.76G/4.97G [01:17<01:15, 42.6MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 1.52G/2.67G [01:17<01:37, 11.8MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 1.55G/2.67G [01:17<01:13, 15.1MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  36% 1.79G/4.97G [01:17<01:13, 43.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  37% 1.85G/4.97G [01:17<00:46, 67.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  38% 1.91G/4.97G [01:18<00:34, 88.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  40% 1.97G/4.97G [01:18<00:25, 116MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 1.61G/2.67G [01:18<00:49, 21.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 1.68G/2.67G [01:19<00:31, 32.1MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 1.74G/2.67G [01:19<00:20, 45.5MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 1.80G/2.67G [01:20<00:15, 54.6MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  41% 2.03G/4.97G [01:20<00:55, 52.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 2.08G/4.97G [01:26<02:08, 22.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 1.87G/2.67G [01:26<00:33, 23.8MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  42% 2.10G/4.97G [01:26<01:55, 24.9MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 1.92G/2.67G [01:26<00:24, 30.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  43% 2.15G/4.97G [01:27<01:31, 30.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  44% 2.21G/4.97G [01:27<01:00, 45.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  45% 2.23G/4.97G [01:27<00:58, 46.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 1.99G/2.67G [01:28<00:19, 34.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  46% 2.28G/4.97G [01:28<00:48, 55.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  47% 2.34G/4.97G [01:34<02:00, 21.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 2.06G/2.67G [01:34<00:31, 19.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  47% 2.35G/4.97G [01:35<02:06, 20.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 2.12G/2.67G [01:35<00:20, 26.3MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  48% 2.37G/4.97G [01:35<01:54, 22.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 2.47G/4.97G [01:35<00:53, 46.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 2.49G/4.97G [01:36<00:51, 48.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 2.19G/2.67G [01:51<00:48, 9.94MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  50% 2.51G/4.97G [01:52<06:16, 6.54MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  52% 2.58G/4.97G [01:52<03:28, 11.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  53% 2.63G/4.97G [01:53<02:25, 16.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  54% 2.70G/4.97G [01:56<01:59, 19.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  55% 2.72G/4.97G [01:56<01:53, 19.8MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 2.24G/2.67G [01:56<00:45, 9.54MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  55% 2.74G/4.97G [01:57<01:44, 21.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  57% 2.84G/4.97G [01:57<00:47, 44.5MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 2.30G/2.67G [01:57<00:27, 13.4MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  58% 2.91G/4.97G [01:59<00:52, 39.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 2.35G/2.67G [02:03<00:27, 11.5MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  60% 2.97G/4.97G [02:05<01:30, 22.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  61% 3.03G/4.97G [02:05<01:00, 32.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  63% 3.11G/4.97G [02:09<01:10, 26.3MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  64% 3.20G/4.97G [02:09<00:43, 40.8MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  66% 3.27G/4.97G [02:13<00:58, 29.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  67% 3.34G/4.97G [02:13<00:41, 39.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  68% 3.39G/4.97G [02:13<00:32, 48.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  69% 3.42G/4.97G [02:13<00:28, 54.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  70% 3.48G/4.97G [02:14<00:19, 75.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  71% 3.54G/4.97G [02:14<00:15, 92.0MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  73% 3.61G/4.97G [02:14<00:12, 110MB/s] \u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 2.42G/2.67G [02:15<00:29, 8.61MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  74% 3.68G/4.97G [02:15<00:10, 123MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  75% 3.75G/4.97G [02:19<00:30, 40.7MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 2.48G/2.67G [02:19<00:18, 10.0MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  76% 3.80G/4.97G [02:19<00:22, 52.4MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 2.55G/2.67G [02:19<00:08, 14.2MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  78% 3.87G/4.97G [02:20<00:17, 64.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  79% 3.94G/4.97G [02:23<00:28, 36.2MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 2.61G/2.67G [02:25<00:04, 12.7MB/s]\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  80% 4.00G/4.97G [02:25<00:26, 36.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  82% 4.07G/4.97G [02:25<00:18, 49.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  83% 4.12G/4.97G [02:26<00:14, 57.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 4.16G/4.97G [02:26<00:13, 60.6MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  84% 4.19G/4.97G [02:27<00:11, 66.9MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  85% 4.24G/4.97G [02:27<00:08, 85.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  86% 4.26G/4.97G [02:27<00:08, 82.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  87% 4.31G/4.97G [02:29<00:12, 53.3MB/s]\u001b[A\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 2.67G/2.67G [02:29<00:00, 17.8MB/s]\n",
            "\n",
            "\n",
            "model-00001-of-00002.safetensors:  88% 4.38G/4.97G [02:29<00:08, 71.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  89% 4.44G/4.97G [02:30<00:06, 79.1MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  91% 4.51G/4.97G [02:32<00:09, 49.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  92% 4.55G/4.97G [02:35<00:13, 30.2MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  94% 4.66G/4.97G [02:36<00:05, 52.7MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  95% 4.73G/4.97G [02:36<00:03, 70.5MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  97% 4.84G/4.97G [02:36<00:01, 98.4MB/s]\u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors:  99% 4.91G/4.97G [02:37<00:00, 116MB/s] \u001b[A\u001b[A\n",
            "\n",
            "model-00001-of-00002.safetensors: 100% 4.97G/4.97G [02:37<00:00, 31.6MB/s]\n",
            "Fetching 2 files: 100% 2/2 [02:37<00:00, 78.91s/it] \n",
            "Loading checkpoint shards: 100% 2/2 [00:41<00:00, 20.59s/it]\n",
            "generation_config.json: 100% 181/181 [00:00<00:00, 1.80MB/s]\n",
            "tokenizer_config.json: 3.44kB [00:00, 891kB/s]\n",
            "tokenizer.model: 100% 500k/500k [00:01<00:00, 269kB/s]\n",
            "tokenizer.json: 1.94MB [00:00, 145MB/s]\n",
            "added_tokens.json: 100% 306/306 [00:00<00:00, 2.39MB/s]\n",
            "special_tokens_map.json: 100% 599/599 [00:00<00:00, 6.04MB/s]\n",
            "[NanoSentri-Train]: Loading and formatting dataset...\n",
            "Generating train split: 1000 examples [00:00, 13941.65 examples/s]\n",
            "Map: 100% 1000/1000 [00:00<00:00, 12899.16 examples/s]\n",
            "[NanoSentri-Train]: Sample formatted entry:\n",
            "<|user|>\n",
            "Analyze the power subsystem logs for the weather station.\n",
            "\n",
            "Technical Context:\n",
            "[2025-12-22T07:04:15] ID:LOG-10000 DEVICE:PWD22 Present Weather Detector >>> V_IN: 8.4V [LOW] | REF_CHK: FAIL <|end|>\n",
            "<|assistant|>\n",
            "The input voltage (V_IN) has dropped to 8.4V, which is below the operating threshold of 10V-30V for the WXT536. This triggers a Reference Check (REF_CHK) failure. Recommendation: Check the solar charge controller or replace the backup battery immediately. <|end|>\n",
            "[NanoSentri-Train]: Tokenizing dataset...\n",
            "Map: 100% 1000/1000 [00:00<00:00, 2576.98 examples/s]\n",
            "/usr/local/lib/python3.12/dist-packages/transformers/training_args.py:2111: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ðŸ¤— Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "Truncating train dataset: 100% 1000/1000 [00:00<00:00, 101434.20 examples/s]\n",
            "[NanoSentri-Train]: Starting training...\n",
            "  0% 0/100 [00:00<?, ?it/s]WARNING:transformers_modules.microsoft.Phi_hyphen_3_hyphen_mini_hyphen_4k_hyphen_instruct.f39ac1d28e925b323eae81227eaba4464caced4e.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n",
            "{'loss': 1.7156, 'grad_norm': 0.9453125, 'learning_rate': 0.000182, 'entropy': 2.1914441764354704, 'num_tokens': 81920.0, 'mean_token_accuracy': 0.6153178334236145, 'epoch': 0.16}\n",
            "{'loss': 0.3772, 'grad_norm': 0.400390625, 'learning_rate': 0.000162, 'entropy': 1.23426910340786, 'num_tokens': 163840.0, 'mean_token_accuracy': 0.8833359479904175, 'epoch': 0.32}\n",
            "{'loss': 0.1617, 'grad_norm': 0.158203125, 'learning_rate': 0.000142, 'entropy': 1.2563822001218796, 'num_tokens': 245760.0, 'mean_token_accuracy': 0.9323714271187782, 'epoch': 0.48}\n",
            "{'loss': 0.1525, 'grad_norm': 0.107421875, 'learning_rate': 0.000122, 'entropy': 1.453241413831711, 'num_tokens': 327680.0, 'mean_token_accuracy': 0.9347789257764816, 'epoch': 0.64}\n",
            "{'loss': 0.1514, 'grad_norm': 0.09814453125, 'learning_rate': 0.00010200000000000001, 'entropy': 1.5221986681222917, 'num_tokens': 409600.0, 'mean_token_accuracy': 0.9344479158520699, 'epoch': 0.8}\n",
            "{'loss': 0.151, 'grad_norm': 0.0888671875, 'learning_rate': 8.2e-05, 'entropy': 1.5449532985687255, 'num_tokens': 491520.0, 'mean_token_accuracy': 0.9353076413273811, 'epoch': 0.96}\n",
            "{'loss': 0.1487, 'grad_norm': 0.08056640625, 'learning_rate': 6.2e-05, 'entropy': 1.5481583225099664, 'num_tokens': 569344.0, 'mean_token_accuracy': 0.935532094616639, 'epoch': 1.11}\n",
            "{'loss': 0.1513, 'grad_norm': 0.09521484375, 'learning_rate': 4.2e-05, 'entropy': 1.5401576429605484, 'num_tokens': 651264.0, 'mean_token_accuracy': 0.9346129655838012, 'epoch': 1.27}\n",
            "{'loss': 0.15, 'grad_norm': 0.0888671875, 'learning_rate': 2.2000000000000003e-05, 'entropy': 1.5534310311079025, 'num_tokens': 733184.0, 'mean_token_accuracy': 0.9347233638167382, 'epoch': 1.43}\n",
            "{'loss': 0.1497, 'grad_norm': 0.142578125, 'learning_rate': 2.0000000000000003e-06, 'entropy': 1.5467238038778306, 'num_tokens': 815104.0, 'mean_token_accuracy': 0.9344969123601914, 'epoch': 1.59}\n",
            "{'train_runtime': 9177.0848, 'train_samples_per_second': 0.174, 'train_steps_per_second': 0.011, 'train_loss': 0.33092414736747744, 'epoch': 1.59}\n",
            "100% 100/100 [2:32:57<00:00, 91.77s/it]\n",
            "[NanoSentri-Train]: Training complete. Saving adapters to phi3-vaisala-adapter\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create a zip file of the adapter folder\n",
        "shutil.make_archive('phi3-vaisala-adapter', 'zip', 'phi3-vaisala-adapter')\n",
        "\n",
        "# Trigger the browser download\n",
        "files.download('phi3-vaisala-adapter.zip')"
      ],
      "metadata": {
        "id": "eg5_QoTamGW3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "1a010050-a3d1-4077-bd31-ec04ff8ba02a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_09ecab60-e855-47c8-97dd-03f6d4a1ce2e\", \"phi3-vaisala-adapter.zip\", 213597451)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}