{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sadYD3P9jy7T",
        "outputId": "eb8e71d9-0d84-49d6-8ba6-27ae9a7aba51"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements_colab.txt\n",
        "torch>=2.2.0\n",
        "transformers>=4.38.0\n",
        "datasets>=2.17.0\n",
        "peft>=0.18.1\n",
        "bitsandbytes>=0.43.0\n",
        "accelerate>=0.30.0\n",
        "scipy>=1.11.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KsnbHed6tq-"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "yKEAi-3qppcG"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Create the directory for the script\n",
        "os.makedirs('src', exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-OPB-oxkVZg",
        "outputId": "91dafb7e-c2ab-4d45-cc4d-e9333b5b43e6"
      },
      "outputs": [],
      "source": [
        "%%writefile src/train_colab.py\n",
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "from trl import SFTTrainer\n",
        "\n",
        "def log(msg):\n",
        "    print(f\"[NanoSentri-Train]: {msg}\")\n",
        "\n",
        "def format_phi3_prompt(sample):\n",
        "    instruction = sample['instruction']\n",
        "    context = sample.get('input', '')\n",
        "    response = sample['output']\n",
        "\n",
        "    if context:\n",
        "        user_content = f\"{instruction}\\n\\nTechnical Context:\\n{context}\"\n",
        "    else:\n",
        "        user_content = instruction\n",
        "\n",
        "    text = f\"<|user|>\\n{user_content} <|end|>\\n<|assistant|>\\n{response} <|end|>\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Phi-3 QLoRA Trainer\")\n",
        "    parser.add_argument(\"--data_path\", type=str, required=True, help=\"Path to JSONL dataset\")\n",
        "    parser.add_argument(\"--output_dir\", type=str, default=\"./phi3-vaisala-adapter\", help=\"Where to save adapters\")\n",
        "    parser.add_argument(\"--base_model\", type=str, default=\"microsoft/Phi-3-mini-4k-instruct\", help=\"HF Model ID\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    log(f\"Initializing Training Pipeline on {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")\n",
        "\n",
        "    log(f\"Loading base model: {args.base_model}...\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.base_model,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"eager\"\n",
        "    )\n",
        "\n",
        "    model.gradient_checkpointing_enable()\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args.base_model, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.unk_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    peft_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "        target_modules=[\"qkv_proj\", \"o_proj\", \"gate_up_proj\", \"down_proj\"]\n",
        "    )\n",
        "\n",
        "    log(\"Loading and formatting dataset...\")\n",
        "    dataset = load_dataset(\"json\", data_files=args.data_path, split=\"train\")\n",
        "    dataset = dataset.map(format_phi3_prompt)\n",
        "\n",
        "    log(f\"Sample formatted entry:\\n{dataset[0]['text']}\")\n",
        "\n",
        "    # Tokenize the dataset with truncation\n",
        "    log(\"Tokenizing dataset...\")\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(\n",
        "            examples[\"text\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=512,\n",
        "            return_tensors=None\n",
        "        )\n",
        "\n",
        "    tokenized_dataset = dataset.map(\n",
        "        tokenize_function,\n",
        "        batched=True,\n",
        "        remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=args.output_dir,\n",
        "        per_device_train_batch_size=4,\n",
        "        gradient_accumulation_steps=4,\n",
        "        learning_rate=2e-4,\n",
        "        logging_steps=10,\n",
        "        max_steps=100,\n",
        "        save_steps=50,\n",
        "        fp16=False,\n",
        "        bf16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        report_to=\"none\",\n",
        "        gradient_checkpointing=True,\n",
        "    )\n",
        "\n",
        "    # Data collator for language modeling\n",
        "    data_collator = DataCollatorForLanguageModeling(\n",
        "        tokenizer=tokenizer,\n",
        "        mlm=False,  # Not masked language modeling\n",
        "    )\n",
        "\n",
        "    # SFTTrainer for TRL 0.7.10 - minimal parameters\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "        peft_config=peft_config,\n",
        "        data_collator=data_collator,\n",
        "    )\n",
        "\n",
        "    log(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    log(f\"Training complete. Saving adapters to {args.output_dir}\")\n",
        "    trainer.model.save_pretrained(args.output_dir)\n",
        "    tokenizer.save_pretrained(args.output_dir)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrHyIqAmkrZA"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements_colab.txt\n",
        "!pip install flash_attn --no-build-isolation # Optional speedup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Wgl1peyl91i"
      },
      "outputs": [],
      "source": [
        "!python src/train_colab.py --data_path \"vaisala_synthetic_train.jsonl\" --output_dir \"phi3-vaisala-adapter\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "eg5_QoTamGW3",
        "outputId": "1a010050-a3d1-4077-bd31-ec04ff8ba02a"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_09ecab60-e855-47c8-97dd-03f6d4a1ce2e\", \"phi3-vaisala-adapter.zip\", 213597451)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import shutil\n",
        "from google.colab import files\n",
        "\n",
        "# Create a zip file of the adapter folder\n",
        "shutil.make_archive('phi3-vaisala-adapter', 'zip', 'phi3-vaisala-adapter')\n",
        "\n",
        "# Trigger the browser download\n",
        "files.download('phi3-vaisala-adapter.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZ_fSQcrntCg"
      },
      "source": [
        "Merge -> Export -> Quantize pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5GeaseFe-U3",
        "outputId": "81c2db8b-a329-4026-e38b-57aabd71edd2"
      },
      "outputs": [],
      "source": [
        "%%writefile requirements_export.txt\n",
        "# Core Stack\n",
        "transformers>=4.41.2\n",
        "peft>=0.11.0\n",
        "accelerate>=0.30.0\n",
        "\n",
        "# Optimum with ONNX Runtime GPU support\n",
        "optimum[onnxruntime-gpu]>=1.20.0\n",
        "\n",
        "# Utilities\n",
        "protobuf>=3.20.3,<6.0.0\n",
        "scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8NLpvMUiId_"
      },
      "outputs": [],
      "source": [
        "# Install everything from requirements\n",
        "!pip install --upgrade --upgrade-strategy eager -r requirements_export.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNe1urWDhgQV",
        "outputId": "6f26cd60-5b4b-42bc-b218-e11fe71fbc9f"
      },
      "outputs": [],
      "source": [
        "%%writefile src/merge.py\n",
        "import argparse\n",
        "import torch\n",
        "import shutil\n",
        "import gc\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# 1. Setup\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--adapter_dir\", type=str, required=True)\n",
        "args = parser.parse_args()\n",
        "\n",
        "base_model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "merged_path = \"./merged_model_tmp\"\n",
        "\n",
        "print(f\">>> [Phase 1] Loading Base Model: {base_model_id}...\")\n",
        "# Load in FP16 to keep RAM usage lower\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(f\">>> [Phase 1] Loading Adapter from {args.adapter_dir}...\")\n",
        "model = PeftModel.from_pretrained(base_model, args.adapter_dir)\n",
        "\n",
        "print(\">>> [Phase 1] Merging weights...\")\n",
        "model = model.merge_and_unload()\n",
        "\n",
        "print(f\">>> [Phase 1] Saving merged model to disk: {merged_path}...\")\n",
        "model.save_pretrained(merged_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, trust_remote_code=True)\n",
        "tokenizer.save_pretrained(merged_path)\n",
        "\n",
        "print(\">>> SUCCESS. Now RESTART your runtime to clear RAM.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uedVD7fdiZ4v"
      },
      "outputs": [],
      "source": [
        "!python src/merge.py --adapter_dir \"phi3-vaisala-adapter\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "bsiSuCSzsyN4",
        "outputId": "b203a44b-6627-4222-b439-6e4a7a64d18e"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "print(\">>> Zipping the merged model (approx 4-5 GB)...\")\n",
        "print(\"    This may take 2-3 minutes to zip and prepare for download.\")\n",
        "\n",
        "# Zip the folder\n",
        "shutil.make_archive(\"phi3_merged_model\", 'zip', \"merged_model_tmp\")\n",
        "\n",
        "print(\">>> Download starting...\")\n",
        "print(\"    ⚠️ IMPORTANT: Do not close this tab until the download finishes.\")\n",
        "files.download(\"phi3_merged_model.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
